{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)          #some code samples return useless future warnings, this suppresses them\n",
    "\n",
    "training = pd.read_csv(\"data/train_set.csv\")\n",
    "test = pd.read_csv(\"data/test_set.csv\")\n",
    "test.dropna(inplace=True)\n",
    "test.drop(columns=\"Unnamed: 0\",inplace=True)\n",
    "training.drop(columns=\"Unnamed: 0\",inplace=True)\n",
    "training.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['buying', ['low', 'vhigh', 'med', 'high']], ['maint', ['med', 'vhigh', 'low', 'high']], ['doors', ['3', '2', '5more', '4']], ['persons', ['4', '2', 'more']], ['lug_boot', ['small', 'med', 'big']], ['safety', ['high', 'low', 'med']], ['rating', ['good', 'unacc', 'vgood', 'acc']]]\n"
    }
   ],
   "source": [
    "vals = [0]*len(training.columns)\n",
    "for i,col in enumerate(training.columns):\n",
    "    vals[i] = [col] + [list(training[col].unique())]\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = [['low','small','unacc','med','acc','big','high','good','vgood','vhigh','more','5more'],[0,0,0,1,1,2,2,2,3,3,5,5]]\n",
    "training.replace(subs[0],subs[1],inplace=True)\n",
    "test.replace(subs[0],subs[1],inplace=True)\n",
    "\n",
    "for col in training.columns:\n",
    "    training[col] = pd.to_numeric(training[col])\n",
    "    test[col] = pd.to_numeric(test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "first search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    7.0s\n[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   19.1s\n[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   33.4s\n[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:   56.7s\n[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:  1.4min finished\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import sqrt\n",
    "NFOLDS = 5\n",
    "predictorCols = ['buying','maint','doors','persons','lug_boot','safety']\n",
    "X = training[predictorCols]\n",
    "y = training['rating']\n",
    "\n",
    "nTrees = [n**2 for n in range(3,16,3)]\n",
    "maxDepth = [None] + list(range(3,16,3))\n",
    "maxFeaturesSplit = list(range(1,len(X.columns)+1))\n",
    "\n",
    "searchParametersCoarse = {'n_estimators': nTrees,'max_depth':maxDepth,'max_features':maxFeaturesSplit}\n",
    "\n",
    "gridSearchKwargs = {'cv':5,'verbose':2,'n_jobs':-1,'scoring':'accuracy','return_train_score':True}\n",
    "rfc = RandomForestClassifier()\n",
    "model = GridSearchCV(rfc,searchParametersCoarse,**gridSearchKwargs)\n",
    "model = model.fit(X,y)\n",
    "accuracies = pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Generating finer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestVals = model.best_params_\n",
    "bestDepth = bestVals['max_depth']\n",
    "bestNTrees = int(sqrt(bestVals['n_estimators']))\n",
    "\n",
    "if(bestDepth is None):\n",
    "    maxDepth = [None]\n",
    "else:\n",
    "    maxDepth = [max(i+bestDepth,1) for i in range(-2,3)]\n",
    "nTrees = [(bestNTrees + i)**2 for i in range(-2,3)]\n",
    "searchParametersFine = {'n_estimators': nTrees,'max_depth':maxDepth,'max_features':maxFeaturesSplit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Second search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.2s\n[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=-1)]: Done 713 tasks      | elapsed:   13.4s\n[Parallel(n_jobs=-1)]: Done 727 out of 750 | elapsed:   13.6s remaining:    0.3s\n[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed:   13.9s finished\n"
    }
   ],
   "source": [
    "model = GridSearchCV(rfc,searchParametersFine,**gridSearchKwargs)\n",
    "model = model.fit(X,y)\n",
    "accuracies = accuracies.append(pd.DataFrame(model.cv_results_))\n",
    "relevantColumns = ['param_max_depth','param_max_features','param_n_estimators','mean_test_score','mean_train_score']\n",
    "accuracies = accuracies[relevantColumns].replace([None],[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.6s\n[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    8.0s finished\n"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "linearSVC = LinearSVC(dual=False)\n",
    "Cs = np.logspace(-2,3,15)\n",
    "losses = ['hinge','squared_hinge']\n",
    "penalties = ['l1','l2']\n",
    "searchParametersCoarse = {'C':Cs,'penalty':penalties}\n",
    "model = GridSearchCV(linearSVC,searchParametersCoarse,**gridSearchKwargs)\n",
    "model = model.fit(X,y)\n",
    "accuracies = pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "bestVals = model.best_params_\n",
    "bestC = log(bestVals['C'],10)\n",
    "\n",
    "Cs = np.logspace(bestC-0.5,bestC+0.5,15)\n",
    "\n",
    "searchParametersFine = {'C':Cs,'penalty':penalties}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.2s\n[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.8s finished\n"
    }
   ],
   "source": [
    "model = GridSearchCV(linearSVC,searchParametersFine,**gridSearchKwargs)\n",
    "model = model.fit(X,y)\n",
    "accuracies = accuracies.append(pd.DataFrame(model.cv_results_))\n",
    "relevantColumns = ['param_C','param_penalty','mean_test_score','mean_train_score']\n",
    "accuracies = accuracies[relevantColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'C': 0.22758459260747887, 'penalty': 'l2'}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "nnScaler = MinMaxScaler()\n",
    "\n",
    "nnScaler = nnScaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "useCuda = torch.cuda.is_available()\n",
    "\n",
    "nPredictors = 6\n",
    "nRatings = 4\n",
    "nEpochs = 120\n",
    "batchSize = 64\n",
    "learningRate = 1e-2\n",
    "\n",
    "class NNShallow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNShallow, self).__init__()\n",
    "        self.layer1 = nn.Linear(nPredictors, 200)\n",
    "        self.layer2 = nn.Linear(200, 200)\n",
    "        self.layer3 = nn.Linear(200, nRatings)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "\n",
    "    def reset(self):\n",
    "        def resetWeight(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.reset_parameters()\n",
    "        self.apply(resetWeight)\n",
    "\n",
    "class NNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        nny = np.array(y,dtype=int)\n",
    "        self.X = torch.from_numpy(nnScaler.transform(X)).float()\n",
    "        self.y = torch.from_numpy(nny).long()\n",
    "\n",
    "        if useCuda:\n",
    "            self.X = self.X.cuda()\n",
    "            self.y = self.y.cuda()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index],self.y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnDataset = NNDataset(X,y)\n",
    "network = NNShallow()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if useCuda:\n",
    "    network.cuda()\n",
    "def getOptLoad(lr,bSize):\n",
    "    optimiser = torch.optim.SGD(network.parameters(),lr=lr)\n",
    "    loader = torch.utils.data.DataLoader(dataset=nnDataset,batch_size=bSize,shuffle=True)\n",
    "    return optimiser, loader\n",
    "optimiser, loader = getOptLoad(learningRate,batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch [120/120]"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for predictors, ratings in loader:\n",
    "\n",
    "        optimiser.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = network(predictors)                     # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, ratings)                # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimiser.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print('\\rEpoch [%d/%d]'%(epoch+1, num_epochs),end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}